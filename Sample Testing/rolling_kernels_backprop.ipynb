{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rolling_kernels_backprop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIZpTxsnjE_0"
      },
      "source": [
        "\n",
        "*This notebook will guide through a toy example, explaing the forward and the backward propagation through the rolling convolution filters.*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eblxKFa3kfsE"
      },
      "source": [
        "# Rolling Convolution Filters : **Back Propagation Using PyTorch**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnpTepq2gvFZ",
        "outputId": "ffb6c824-a01a-4784-ffbc-4c2bd743d637"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "\n",
        "# define a sample input x\n",
        "x = torch.tensor([[[1,2,3],[4,5,6],[7,8,9]],[[9,-8,7],[9,8,7],[3,-2,-1]]]).unsqueeze(0)\n",
        "x = x.float()\n",
        "x.requires_grad = True\n",
        "print(\"x=\\n\\n\", x)\n",
        "print(\"\\n\")\n",
        "print(\"Shape of x : \",x.shape)\n",
        "print('\\n\\n')\n",
        "# define a sample weight w\n",
        "w = nn.Parameter(torch.tensor([[[1,-1], [-1,1]],[[1,2], [-1,-2]]]).unsqueeze(0).float())\n",
        "print(\"w=\\n\\n\",w)\n",
        "print(\"\\n\")\n",
        "print(\"Shape of w : \",w.shape)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x=\n",
            "\n",
            " tensor([[[[ 1.,  2.,  3.],\n",
            "          [ 4.,  5.,  6.],\n",
            "          [ 7.,  8.,  9.]],\n",
            "\n",
            "         [[ 9., -8.,  7.],\n",
            "          [ 9.,  8.,  7.],\n",
            "          [ 3., -2., -1.]]]], requires_grad=True)\n",
            "\n",
            "\n",
            "Shape of x :  torch.Size([1, 2, 3, 3])\n",
            "\n",
            "\n",
            "\n",
            "w=\n",
            "\n",
            " Parameter containing:\n",
            "tensor([[[[ 1., -1.],\n",
            "          [-1.,  1.]],\n",
            "\n",
            "         [[ 1.,  2.],\n",
            "          [-1., -2.]]]], requires_grad=True)\n",
            "\n",
            "\n",
            "Shape of w :  torch.Size([1, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km_DxvkGnCwQ",
        "outputId": "e4aa0626-9f78-433f-fb42-e359825e4cfe"
      },
      "source": [
        "# define the rolling convolution\n",
        "\n",
        "def roll(input, weight, stride = 1, padding=0):   \n",
        "    w = weight\n",
        "    s = stride\n",
        "    p = padding\n",
        "    rolls = w.size()[1]    \n",
        "    for i in range(1,rolls+1):\n",
        "      if i==1:\n",
        "        filter = w\n",
        "      else:\n",
        "        w = torch.roll(w, shifts = 1,  dims=1)\n",
        "        filter = torch.cat((filter, w), dim=0)\n",
        "    return F.relu(F.conv2d(input,filter,None,s,p))\n",
        "\n",
        "# perform the rolling convolution operation\n",
        "\n",
        "out1 = roll(x,w)\n",
        "print(\"out1=\\n\\n\",out1)\n",
        "print(\"\\n\")\n",
        "print(\"Shape of out1 : \",out1.shape)\n",
        "\n",
        "# get a scalar output by using mean operator\n",
        "\n",
        "print(\"\\n\")\n",
        "out2 = out1.mean()\n",
        "print(\"out2=\\n\\n\",out2)\n",
        "print(\"\\n\")\n",
        "\n",
        "# let y = 10 be the groundtruth, assume mse loss\n",
        "\n",
        "y =10\n",
        "\n",
        "# compute loss and then perform back propagation\n",
        "((y - out2)**2).backward()\n",
        "\n",
        "# print gradients wrt w\n",
        "print(\"Grad wrt w :\\n\\n \",w.grad.data)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# print gradients wrt x\n",
        "print(\"Grad wrt x :\\n\\n \",x.grad.data)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out1=\n",
            "\n",
            " tensor([[[[ 0.,  0.],\n",
            "          [26., 26.]],\n",
            "\n",
            "         [[ 7.,  0.],\n",
            "          [ 0.,  0.]]]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "\n",
            "Shape of out1 :  torch.Size([1, 2, 2, 2])\n",
            "\n",
            "\n",
            "out2=\n",
            "\n",
            " tensor(7.3750, grad_fn=<MeanBackward0>)\n",
            "\n",
            "\n",
            "Grad wrt w :\n",
            "\n",
            "  tensor([[[[-11.8125,  -1.9688],\n",
            "          [-15.7500, -16.4062]],\n",
            "\n",
            "         [[-11.8125, -11.1562],\n",
            "          [ -3.2812,  -1.3125]]]])\n",
            "\n",
            "\n",
            "Grad wrt x :\n",
            "\n",
            "  tensor([[[[-0.6562, -1.3125,  0.0000],\n",
            "          [ 0.0000,  1.3125,  0.6562],\n",
            "          [ 0.6562,  0.0000, -0.6562]],\n",
            "\n",
            "         [[-0.6562,  0.6562,  0.0000],\n",
            "          [ 0.0000, -2.6250, -1.3125],\n",
            "          [ 0.6562,  1.9688,  1.3125]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRqF6KyVBKc7"
      },
      "source": [
        "# Rolling Convolution Filters : **User Defined Back Propagation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPO5u_6vBlY2",
        "outputId": "b4ff1948-ab19-44d1-8f76-830cc3e1e9b4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "\n",
        "# define a sample input x\n",
        "x = torch.tensor([[[1,2,3],[4,5,6],[7,8,9]],[[9,-8,7],[9,8,7],[3,-2,-1]]]).unsqueeze(0)\n",
        "x = x.float()\n",
        "x.requires_grad = True\n",
        "print(\"x=\\n\\n\", x)\n",
        "print(\"\\n\")\n",
        "print(\"Shape of x : \",x.shape)\n",
        "print('\\n\\n')\n",
        "# define a sample weight w\n",
        "w = nn.Parameter(torch.tensor([[[1,-1], [-1,1]],[[1,2], [-1,-2]]]).unsqueeze(0).float())\n",
        "print(\"w=\\n\\n\",w)\n",
        "print(\"\\n\")\n",
        "print(\"Shape of w : \",w.shape)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x=\n",
            "\n",
            " tensor([[[[ 1.,  2.,  3.],\n",
            "          [ 4.,  5.,  6.],\n",
            "          [ 7.,  8.,  9.]],\n",
            "\n",
            "         [[ 9., -8.,  7.],\n",
            "          [ 9.,  8.,  7.],\n",
            "          [ 3., -2., -1.]]]], requires_grad=True)\n",
            "\n",
            "\n",
            "Shape of x :  torch.Size([1, 2, 3, 3])\n",
            "\n",
            "\n",
            "\n",
            "w=\n",
            "\n",
            " Parameter containing:\n",
            "tensor([[[[ 1., -1.],\n",
            "          [-1.,  1.]],\n",
            "\n",
            "         [[ 1.,  2.],\n",
            "          [-1., -2.]]]], requires_grad=True)\n",
            "\n",
            "\n",
            "Shape of w :  torch.Size([1, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeN7teOdG9NS",
        "outputId": "021ef13e-1dfa-4af5-f787-e9d7386c89d1"
      },
      "source": [
        "# define the rolling convolution\n",
        "\n",
        "def roll(input, weight, stride = 1, padding=0):   \n",
        "    w = weight\n",
        "    s = stride\n",
        "    p = padding\n",
        "    rolls = w.size()[1]    \n",
        "    for i in range(1,rolls+1):\n",
        "      if i==1:\n",
        "        filter = w\n",
        "      else:\n",
        "        w = torch.roll(w, shifts = 1,  dims=1)\n",
        "        filter = torch.cat((filter, w), dim=0)\n",
        "    return F.relu(F.conv2d(input,filter,None,s,p))\n",
        "\n",
        "# perform the rolling convolution operation\n",
        "\n",
        "out1 = roll(x,w)\n",
        "print(\"out1=\\n\\n\",out1)\n",
        "print(\"\\n\")\n",
        "print(\"Shape of out1 : \",out1.shape)\n",
        "\n",
        "# get a scalar output by using mean operator\n",
        "\n",
        "print(\"\\n\")\n",
        "out2 = out1.mean()\n",
        "print(\"out2=\\n\\n\",out2)\n",
        "print(\"\\n\")\n",
        "\n",
        "# let y = 10 be the groundtruth, assume mse loss\n",
        "\n",
        "y =10\n",
        "\n",
        "# grad wrt out2\n",
        "\n",
        "grad_out2 = -1 * 2 * (y - out2)\n",
        "#print(grad_out2)\n",
        "\n",
        "temp = (grad_out2/8).item()\n",
        "#print(temp)\n",
        "\n",
        "# grad wrt out1\n",
        "\n",
        "grad_out1 = torch.tensor([[[[temp, temp], [temp,temp]], [[temp,temp],[temp, temp]]]])\n",
        "# print(grad_out1.shape)\n",
        "\n",
        "# grad after crossing relu = [grad_y1, grad_y2] : these are gradients wrt feature maps\n",
        "\n",
        "grad_relu = ((out1>0) * grad_out1).float()\n",
        "\n",
        "# grad wrt W (channel 1)\n",
        "\n",
        "# grad_w1 = x_channel_1 * grad_y1 + x_channel_2 * grad_y2 (note that x_channel_2 can be seen as the first channel of phi(x), where phi is the channel roll operator)\n",
        "\n",
        "grad_w1 = nn.functional.conv2d((x[0,0,:,:].unsqueeze(0)).unsqueeze(0),(grad_relu[0,0,:,:].unsqueeze(0)).unsqueeze(0),None,1,0) +  nn.functional.conv2d((x[0,1,:,:].unsqueeze(0)).unsqueeze(0),(grad_relu[0,1,:,:].unsqueeze(0)).unsqueeze(0),None,1,0)\n",
        "\n",
        "# grad wrt W (channel 2)\n",
        "\n",
        "# grad_w2 = x_channel_2 * grad_y1 + x_channel_1 * grad_y2 (note that x_channel_1 can be seen as the second channel of phi(x), where phi is the channel roll operator)\n",
        "\n",
        "grad_w2 = nn.functional.conv2d((x[0,1,:,:].unsqueeze(0)).unsqueeze(0),(grad_relu[0,0,:,:].unsqueeze(0)).unsqueeze(0),None,1,0) +  nn.functional.conv2d((x[0,0,:,:].unsqueeze(0)).unsqueeze(0),(grad_relu[0,1,:,:].unsqueeze(0)).unsqueeze(0),None,1,0)\n",
        "# print(grad_w2.data)\n",
        "\n",
        "print(\"Grad wrt w :\\n\\n \", torch.cat((grad_w1.data, grad_w2.data),dim=0))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# grad wrt x (channel 1)\n",
        "\n",
        "# grad_x1 = flip(w_channel_1) * grad_y1 + flip(w_channel_2) * grad_y2 (note that w_channel_2 can be seen as the first channel of phi(w), phi is the channel roll operator)\n",
        "\n",
        "grad_x1 = nn.functional.conv2d((grad_relu[0,0,:,:].unsqueeze(0)).unsqueeze(0), (torch.flip(w[0,0,:,:],[0,1]).unsqueeze(0)).unsqueeze(0),None,1,2) +  nn.functional.conv2d((grad_relu[0,1,:,:].unsqueeze(0)).unsqueeze(0), (torch.flip(w[0,1,:,:],[0,1]).unsqueeze(0)).unsqueeze(0),None,1,2)\n",
        "grad_x1 = grad_x1[:,:,1:4,1:4]\n",
        "\n",
        "# grad wrt x (channel 2)\n",
        "\n",
        "# grad_x2 = flip(w_channel_2) * grad_y1 + flip(w_channel_1) * grad_y2 (note that w_channel_1 can be seen as the second channel of phi(w), phi is the channel roll operator)\n",
        "\n",
        "grad_x2 = nn.functional.conv2d((grad_relu[0,0,:,:].unsqueeze(0)).unsqueeze(0), (torch.flip(w[0,1,:,:],[0,1]).unsqueeze(0)).unsqueeze(0),None,1,2) +  nn.functional.conv2d((grad_relu[0,1,:,:].unsqueeze(0)).unsqueeze(0), (torch.flip(w[0,0,:,:],[0,1]).unsqueeze(0)).unsqueeze(0),None,1,2)\n",
        "grad_x2 = grad_x2[:,:,1:4,1:4]\n",
        "\n",
        "print(\"Grad wrt x :\\n\\n \", torch.cat((grad_x1.data, grad_x2.data),dim=0))\n",
        "\n",
        "print(\"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out1=\n",
            "\n",
            " tensor([[[[ 0.,  0.],\n",
            "          [26., 26.]],\n",
            "\n",
            "         [[ 7.,  0.],\n",
            "          [ 0.,  0.]]]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "\n",
            "Shape of out1 :  torch.Size([1, 2, 2, 2])\n",
            "\n",
            "\n",
            "out2=\n",
            "\n",
            " tensor(7.3750, grad_fn=<MeanBackward0>)\n",
            "\n",
            "\n",
            "Grad wrt w :\n",
            "\n",
            "  tensor([[[[-11.8125,  -1.9688],\n",
            "          [-15.7500, -16.4062]]],\n",
            "\n",
            "\n",
            "        [[[-11.8125, -11.1562],\n",
            "          [ -3.2812,  -1.3125]]]])\n",
            "\n",
            "\n",
            "Grad wrt x :\n",
            "\n",
            "  tensor([[[[-0.6562, -1.3125,  0.0000],\n",
            "          [ 0.0000,  1.3125,  0.6562],\n",
            "          [ 0.6562,  0.0000, -0.6562]]],\n",
            "\n",
            "\n",
            "        [[[-0.6562,  0.6562,  0.0000],\n",
            "          [ 0.0000, -2.6250, -1.3125],\n",
            "          [ 0.6562,  1.9688,  1.3125]]]])\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}